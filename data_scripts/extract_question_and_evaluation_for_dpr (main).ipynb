{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac798c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc1680",
   "metadata": {},
   "source": [
    "# Generate questions for DPR inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f77bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/rawdata/train_dataset.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"../data/rawdata/dev_dataset.json\") as f:\n",
    "    dev_data = json.load(f)\n",
    "with open(\"../data/open_setting_data/dev_data_shared_entities_ranked.json\") as f:\n",
    "    dev_open_data = json.load(f)\n",
    "with open(\"../data/rawdata/test_dataset_closed.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "with open(\"../data/rawdata/train_evi.json\") as f:\n",
    "    train_evi = json.load(f)\n",
    "with open(\"../data/rawdata/dev_evi.json\") as f:\n",
    "    dev_evi = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4b5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/open_setting_data/test_data_shared_entities_ranked.json\") as f:\n",
    "    test_open_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc253d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = {\n",
    "    \"naive\": \"what is the relation between {head} and {tail}?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f90a47",
   "metadata": {},
   "source": [
    "# Wikidata query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d7bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_entity_name(query):\n",
    "    API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',\n",
    "        'type': None if \"Q\" in query else 'property',\n",
    "        'format': 'json',\n",
    "        'language': 'en',\n",
    "        'search': query\n",
    "    }\n",
    "    r = requests.get(API_ENDPOINT, params = params)\n",
    "    return r.json()['search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e9019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = set()\n",
    "for idx, each in enumerate(train_data):\n",
    "    h, t = each[0].split(\"#\")\n",
    "    entities.add(h)\n",
    "    entities.add(t)\n",
    "for idx, each in enumerate(dev_data):\n",
    "    h, t = each[0].split(\"#\")\n",
    "    entities.add(h)\n",
    "    entities.add(t)\n",
    "for idx, each in enumerate(test_data):\n",
    "    entities.add(each['h_id'])\n",
    "    entities.add(each['t_id'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd80be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11971/11971 [46:25<00:00,  4.30it/s]\n"
     ]
    }
   ],
   "source": [
    "entities2name = {}\n",
    "ood = []\n",
    "for each in tqdm(entities):\n",
    "    res = get_entity_name(each)\n",
    "    if len(res) > 0 and 'label' in res[0]:\n",
    "        entities2name[each] = res[0]['label']\n",
    "    else:\n",
    "        ood.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc35251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q61746295',\n",
       " 'Q1979',\n",
       " 'Q962190',\n",
       " 'Q23306030',\n",
       " 'Q1974',\n",
       " 'Q25349812',\n",
       " 'Q51590545',\n",
       " 'Q311555',\n",
       " 'Q3030436',\n",
       " 'Q7125563',\n",
       " 'Q57422063',\n",
       " 'Q18358526',\n",
       " 'Q42888166',\n",
       " 'Q18528831',\n",
       " 'Q75932823',\n",
       " 'Q6214685',\n",
       " 'Q58652618',\n",
       " 'Q27150222',\n",
       " 'Q1028086',\n",
       " 'Q2534695',\n",
       " 'Q5967378',\n",
       " 'Q5146134']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ac4e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities2name['Q18528831'] = 'Macleans of Duart'\n",
    "entities2name['Q1974'] = 'British Columbia'\n",
    "entities2name['Q6214685'] = 'Jogamaya Devi College'\n",
    "entities2name['Q962190'] = 'Eadburh'\n",
    "entities2name['Q2534695'] = 'boxwood'\n",
    "entities2name['Q1028086'] = 'Khanbaliq'\n",
    "entities2name['Q61746295'] = 'Nickol Bay'\n",
    "entities2name['Q57422063'] = 'Plautia'\n",
    "entities2name['Q3030436'] = 'Disney Interactive'\n",
    "entities2name['Q75932823'] = 'Eleanor le Despenser'\n",
    "entities2name['Q27150222'] = 'fair ground'\n",
    "entities2name['Q5967378'] = 'speculative fiction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87cec02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_info = requests.get(\n",
    "    \"https://www.wikidata.org/w/api.php?action=wbgetentities&ids=\" + \"|\".join(ood) + \"&format=json&language=en\"\n",
    ").json()['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ea145f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q25349812 {'id': 'Q25349812', 'missing': ''}\n",
      "Q7125563 {'id': 'Q7125563', 'missing': ''}\n",
      "Q18358526 {'id': 'Q18358526', 'missing': ''}\n",
      "Q42888166 {'id': 'Q42888166', 'missing': ''}\n"
     ]
    }
   ],
   "source": [
    "for key, info in ood_info.items():\n",
    "    if 'labels' in info:\n",
    "        entities2name[key] = info['labels']['en']['value']\n",
    "    else:\n",
    "        print(key, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11a55d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities2name['Q25349812'] = 'Sanctuary Wood'\n",
    "entities2name['Q7125563'] = 'Pakistan Democratic Party'\n",
    "entities2name['Q18358526'] = 'Ossama Youssef'\n",
    "entities2name['Q42888166'] = 'Target Travel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c818b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/q2t.json\") as f:\n",
    "    q2t = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b691d32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Home Park': 4, 'Stagecoach South West': 4, 'Plympton': 4}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2t['Q42888166']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "222068bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "redisd = redis.Redis(host='localhost', port=6379, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5adb6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Travel\n"
     ]
    }
   ],
   "source": [
    "data = json.loads(redisd.get('codred-doc-Home Park'))\n",
    "for ent in data['entities']:\n",
    "    if 'Q' in ent and ent['Q'] == 42888166:\n",
    "        print(ent['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00c18828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11971, 11971)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities), len(entities2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11193ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/q2name.json\", \"w\") as f:\n",
    "    json.dump(entities2name, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17fc0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation2name = {}\n",
    "with open(\"../data/rawdata/relations.json\") as f:\n",
    "    relations = json.load(f)\n",
    "for r in relations:\n",
    "    relation2name[r] = get_entity_name(r)[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a47dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/r2name.json\", \"w\") as f:\n",
    "    json.dump(relation2name, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e3a8a",
   "metadata": {},
   "source": [
    "# Generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c01543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/q2name.json\") as f:\n",
    "    entities2name = json.load(f)\n",
    "with open(\"../data/r2name.json\") as f:\n",
    "    relation2name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0204740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77940/77940 [00:19<00:00, 230285.43it/s]"
     ]
    }
   ],
   "source": [
    "codred_question_open_test_dataset = []\n",
    "pbar = tqdm(total=len(test_open_data))\n",
    "for idx, each in enumerate(test_open_data):\n",
    "    h, t = each[0].split(\"#\")\n",
    "    h_name = entities2name[h]\n",
    "    t_name = entities2name[t]\n",
    "    \n",
    "    r = each[3]\n",
    "    if r == 'n/a':\n",
    "        r_name = 'not available'\n",
    "    else:\n",
    "        r_name = relation2name[r]\n",
    "    record = {\n",
    "        \"question\": question_template['naive'].format(head=h_name, tail=t_name),\n",
    "        \"answers\": [r_name],\n",
    "        \"id\": idx\n",
    "    }\n",
    "    codred_question_open_test_dataset.append(record)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838f341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/DPR/inference/codred_questions_open_test_dataset.jsonl\", \"w\") as f:\n",
    "    for each in codred_question_open_test_dataset:\n",
    "        f.write(json.dumps(each) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c29d067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78023/78023 [00:26<00:00, 2948.00it/s]\u001b[A\n",
      "\n",
      " 33%|████████████████████████████████████████████████████▎                                                                                                        | 25972/78023 [00:00<00:00, 259715.91it/s]\u001b[A\n",
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 64879/78023 [00:00<00:00, 335805.78it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "codred_question_open_dev_dataset = []\n",
    "pbar = tqdm(total=len(dev_open_data))\n",
    "for idx, each in enumerate(dev_open_data):\n",
    "    h, t = each[0].split(\"#\")\n",
    "    h_name = entities2name[h]\n",
    "    t_name = entities2name[t]\n",
    "    \n",
    "    r = each[3]\n",
    "    if r == 'n/a':\n",
    "        r_name = 'not available'\n",
    "    else:\n",
    "        r_name = relation2name[r]\n",
    "    record = {\n",
    "        \"question\": question_template['naive'].format(head=h_name, tail=t_name),\n",
    "        \"answers\": [r_name],\n",
    "        \"id\": idx\n",
    "    }\n",
    "    codred_question_open_dev_dataset.append(record)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84042f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/DPR/inference/codred_questions_open_dev_dataset.jsonl\", \"w\") as f:\n",
    "    for each in codred_question_open_dev_dataset:\n",
    "        f.write(json.dumps(each) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e13911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 101562/129548 [00:00<00:00, 146543.25it/s]"
     ]
    }
   ],
   "source": [
    "codred_question_train_dataset = []\n",
    "pbar = tqdm(total=len(train_data))\n",
    "for idx, each in enumerate(train_data):\n",
    "    h, t = each[0].split(\"#\")\n",
    "    h_name = entities2name[h]\n",
    "    t_name = entities2name[t]\n",
    "    \n",
    "    r = each[3]\n",
    "    if r == 'n/a':\n",
    "        r_name = 'not available'\n",
    "    else:\n",
    "        r_name = relation2name[r]\n",
    "    record = {\n",
    "        \"question\": question_template['naive'].format(head=h_name, tail=t_name),\n",
    "        \"answers\": [r_name],\n",
    "        \"id\": idx\n",
    "    }\n",
    "    codred_question_train_dataset.append(record)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d51821",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/DPR/inference/codred_questions_train_dataset.jsonl\", \"w\") as f:\n",
    "    for each in codred_question_train_dataset:\n",
    "        f.write(json.dumps(each) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c9ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 129548/129548 [00:06<00:00, 19559.87it/s]\u001b[A\n",
      "\n",
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                | 28091/40740 [00:00<00:00, 280903.57it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "codred_question_dev_dataset = []\n",
    "pbar = tqdm(total=len(dev_data))\n",
    "for idx, each in enumerate(dev_data):\n",
    "    h, t = each[0].split(\"#\")\n",
    "    h_name = entities2name[h]\n",
    "    t_name = entities2name[t]\n",
    "    \n",
    "    r = each[3]\n",
    "    if r == 'n/a':\n",
    "        r_name = 'not available'\n",
    "    else:\n",
    "        r_name = relation2name[r]\n",
    "    record = {\n",
    "        \"question\": question_template['naive'].format(head=h_name, tail=t_name),\n",
    "        \"answers\": [r_name],\n",
    "        \"id\": idx\n",
    "    }\n",
    "    codred_question_dev_dataset.append(record)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5dfa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/DPR/inference/codred_questions_dev_dataset.jsonl\", \"w\") as f:\n",
    "    for each in codred_question_dev_dataset:\n",
    "        f.write(json.dumps(each) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32a25c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                             | 0/40524 [00:12<?, ?it/s]\u001b[A\n",
      "\n",
      " 52%|█████████████████████████████████████████████████████████████████████████████████▋                                                                           | 21077/40524 [00:00<00:00, 210768.19it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "codred_question_test_dataset = []\n",
    "pbar = tqdm(total=len(test_data))\n",
    "for idx, each in enumerate(test_data):\n",
    "    h, t = each[0].split(\"#\")\n",
    "    h_name = entities2name[h]\n",
    "    t_name = entities2name[t]\n",
    "    \n",
    "    r_name = 'unk'\n",
    "    record = {\n",
    "        \"question\": question_template['naive'].format(head=h_name, tail=t_name),\n",
    "        \"answers\": [r_name],\n",
    "        \"id\": idx\n",
    "    }\n",
    "    codred_question_test_dataset.append(record)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf4c88e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40524/40524 [00:16<00:00, 210768.19it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "with open(\"../data/DPR/inference/codred_questions_test_dataset.jsonl\", \"w\") as f:\n",
    "    for each in codred_question_test_dataset:\n",
    "        f.write(json.dumps(each) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59341778",
   "metadata": {},
   "source": [
    "# Generate DPR finetune data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f730184",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/rawdata/train_evi.json\") as f:\n",
    "    train_evi = json.load(f)\n",
    "with open(\"../data/rawdata/dev_evi.json\") as f:\n",
    "    dev_evi = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb13b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': 'Luxeuil Abbey',\n",
       " 't': 'Catholic Encyclopedia',\n",
       " 'r': 'P1343',\n",
       " 'doc_h': 'Columbanus',\n",
       " 'doc_t': 'Quapaw',\n",
       " 'evis_h': [[0, 0]],\n",
       " 'evis_t': [[22, 0], [26, 0]],\n",
       " 'id': 29784012,\n",
       " 'key': 'Q1232116#Q302556'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_evi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07bd0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = pd.read_csv(\"../data/codred_passages.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37184a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         text          title\n",
      "id                                                                          \n",
      "12_0        Anarchism is a radical political movement that...      Anarchism\n",
      "12_1        Anarchism 's timeline stretches back to prehis...      Anarchism\n",
      "12_2        Anarchism employ various tactics in order to m...      Anarchism\n",
      "12_3        The etymological origin of the word `` anarchi...      Anarchism\n",
      "12_4        The first political philosopher to call himsel...      Anarchism\n",
      "...                                                       ...            ...\n",
      "63097069_2  Cantor is a career member of the Senior Execut...  Carmen Cantor\n",
      "63097069_3  On July 15 , 2019 , President Trump announced ...  Carmen Cantor\n",
      "63097069_4  On October 16 , 2019 , she appeared before the...  Carmen Cantor\n",
      "63097069_5  Cantor is fluent in Spanish . She is married t...  Carmen Cantor\n",
      "63149555_0                       Safiye Sultan may refer to :  Safiye Sultan\n",
      "\n",
      "[5193458 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5193458it [03:18, 26229.47it/s]\n"
     ]
    }
   ],
   "source": [
    "passage_dict = {}\n",
    "passages.set_index(\"id\", inplace=True)\n",
    "print(passages)\n",
    "for idx, (text, title) in tqdm(passages.iterrows()):\n",
    "    passage_dict[idx] = (title, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5b43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/title2id.json\") as f:\n",
    "    title2id = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec86f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/r2name.json\") as f:\n",
    "    r2name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40de851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2passage = {}\n",
    "for k in passage_dict.keys():\n",
    "    doc_id = k.split(\"_\")[0]\n",
    "    if doc_id not in doc2passage:\n",
    "        doc2passage[doc_id] = []\n",
    "    doc2passage[doc_id].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b00097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data, sample_size=5):\n",
    "    outputs = []\n",
    "    for sample in tqdm(data):\n",
    "        output = {}\n",
    "        \n",
    "        output[\"question\"] = question_template[\"naive\"].format(head=sample[\"h\"], tail=sample[\"t\"])\n",
    "        output[\"answers\"] = [r2name[sample[\"r\"]]]\n",
    "    \n",
    "        output[\"positive_ctxs\"] = []\n",
    "        pos_set = set()\n",
    "        for (pid, _) in sample[\"evis_h\"]:\n",
    "            if pid == -1:\n",
    "                output[\"positive_ctxs\"].append({\n",
    "                    \"title\": sample['doc_h'],\n",
    "                    \"text\": sample['doc_h']\n",
    "                })\n",
    "            else:\n",
    "                lid = f\"{title2id[sample['doc_h']]}_{pid}\"\n",
    "                pos_set.add(lid)\n",
    "                title, text = passage_dict[lid]\n",
    "                output[\"positive_ctxs\"].append({\n",
    "                    \"title\": title,\n",
    "                    \"text\": text\n",
    "                })\n",
    "        for (pid, _) in sample[\"evis_t\"]:\n",
    "            if pid == -1:\n",
    "                output[\"positive_ctxs\"].append({\n",
    "                    \"title\": sample['doc_t'],\n",
    "                    \"text\": sample['doc_t']\n",
    "                })\n",
    "            else:\n",
    "                lid = f\"{title2id[sample['doc_t']]}_{pid}\"\n",
    "                pos_set.add(lid)\n",
    "                title, text = passage_dict[lid]\n",
    "                output[\"positive_ctxs\"].append({\n",
    "                    \"title\": title,\n",
    "                    \"text\": text\n",
    "                })\n",
    "    \n",
    "        p_h = doc2passage[title2id[sample['doc_h']]]\n",
    "        p_t = doc2passage[title2id[sample['doc_t']]]\n",
    "        neg_set = set(random.sample(p_h, min(sample_size, len(p_h))) +\\\n",
    "            random.sample(p_t, min(sample_size, len(p_t))))\n",
    "        neg_set = neg_set.difference(pos_set)\n",
    "    \n",
    "        output[\"negative_ctxs\"] = []\n",
    "        for lid in neg_set:\n",
    "            title, text = passage_dict[lid]\n",
    "            output[\"negative_ctxs\"].append({\n",
    "                \"title\": title,\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "        output[\"hard_negative_ctxs\"] = []\n",
    "        outputs.append(output)\n",
    "        \n",
    "        for pos_ctx in output['positive_ctxs']:\n",
    "            augmented_question = output['question'] + \" \" + pos_ctx['text']\n",
    "            augmented_output = {\n",
    "                'question': augmented_question,\n",
    "                'positive_ctxs': [each for each in output['positive_ctxs'] if each != pos_ctx]\n",
    "            }\n",
    "            \n",
    "            p_h = doc2passage[title2id[sample['doc_h']]]\n",
    "            p_t = doc2passage[title2id[sample['doc_t']]]\n",
    "            neg_set = set(random.sample(p_h, min(sample_size, len(p_h))) +\\\n",
    "                random.sample(p_t, min(sample_size, len(p_t))))\n",
    "            neg_set = neg_set.difference(pos_set)\n",
    "    \n",
    "            augmented_output[\"negative_ctxs\"] = []\n",
    "            for lid in neg_set:\n",
    "                title, text = passage_dict[lid]\n",
    "                augmented_output[\"negative_ctxs\"].append({\n",
    "                    \"title\": title,\n",
    "                    \"text\": text\n",
    "                })\n",
    "            outputs.append(augmented_output)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7218df47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12013/12013 [00:01<00:00, 6204.73it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3497/3497 [00:00<00:00, 4792.96it/s]\n"
     ]
    }
   ],
   "source": [
    "train_outputs = transform(train_evi)\n",
    "dev_outputs = transform(dev_evi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "749d390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/DPR/finetune/train.json\", \"w\") as f:\n",
    "    json.dump(train_outputs, f)\n",
    "with open(\"../data/DPR/finetune/dev.json\", \"w\") as f:\n",
    "    json.dump(dev_outputs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
